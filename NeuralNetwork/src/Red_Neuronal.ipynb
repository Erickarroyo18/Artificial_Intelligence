{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7a6WBLxsAbt"
      },
      "source": [
        "# Recurso para graficar areas de clasificación\n",
        "\n",
        "Es necesario importar este recurso para poder visualizar. En caso de error entrar al link del repo y seguir los pasos para la instalación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1bUxRefPqya",
        "outputId": "ffbdd0ad-58bc-4683-bb29-735ff748a2dc"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from pathlib import Path \n",
        "if Path(\"helper_functions.py\").is_file():\n",
        "  print(\"helper_functions.py already exists, skipping download\")\n",
        "else:\n",
        "  print(\"Downloading helper_functions.py\")\n",
        "  request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")\n",
        "  with open(\"helper_functions.py\", \"wb\") as f:\n",
        "    f.write(request.content)\n",
        "from helper_functions import plot_predictions, plot_decision_boundary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9nSEAWI4nof"
      },
      "source": [
        "# Perceptrón multicapa\n",
        "\n",
        "**Autor:** _Benjamin Torres_\n",
        "\n",
        "Marzo 2019\n",
        "\n",
        "En esta práctica implementarás distintos modelos de perceptrón multicapa para clasificar datos haciendo uso de la biblioteca Pytorch.\n",
        "\n",
        "Puedes encontrar la documentación de esta biblioteca en :\n",
        "[pytorch.org](https://pytorch.org/docs/stable/index.html)\n",
        "\n",
        "Escencialmente, PyTorch se encarga de realizar operaciones sobre **tensores**, los cuales pueden crearse haciendo uso de torch.Tensor y pasando como argumento una lista de Python, por ejemplo:\n",
        "\n",
        "```\n",
        "import torch\n",
        "a = torch.Tensor([[1,0,0],[0,1,0],[0,0,1]])\n",
        "```\n",
        "Crea una matriz identidad de 3x3.\n",
        "\n",
        "Podemos ejecutar operaciones con estos tensores de manera similar a como trabajamos con arreglos de Numpy, por ejemplo: \n",
        "\n",
        "*   Suma, con el método add() o haciendo uso del operador +\n",
        "*   Producto cruz, con el método dot() o con el operador @ \n",
        "*   Producto entrada por entrada con el operador *\n",
        "*   Valor máximo, con argmax()\n",
        "*   Cambio de forma con view( (new shape) )\n",
        "\n",
        "Entre las ventajas más importantes de hacer uso de esta biblioteca está la capacidad que tiene para realizar **diferenciación automática**, que permite obtener gradientes de expresiones fácilmente, lo cual es especialmente importante en redes neuronales."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CyBRuGzjTvxe"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "#!pip install --force https://github.com/chengs/tqdm/archive/colab.zip\n",
        "from tqdm import tnrange"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_-xhrpqpQ5_"
      },
      "source": [
        "Para la implementación de redes neuronales podemos hacer uso de la clase nn.Module.  Ésta se debe extender y se implementa el constructor, así como la manera de hacer *feed forward* con valores de entrada.  Las capas de la red se definen utilizando objetos nn.Linear.\n",
        "\n",
        "## Ejemplo: Perceptrón simple\n",
        "\n",
        "En la siguiente celda puedes encontrar un ejemplo de la implementación de un perceptrón que simula la compuerta AND."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rT8xE8XlpP0u"
      },
      "outputs": [],
      "source": [
        "class Perceptron(nn.Module):\n",
        "    def __init__(self, input_size, output_size, bias=True):\n",
        "      ''' Puedes definir tantos parámetros de entrada como necesites, en este caso únicamente\n",
        "      el número de elementos de entrada y de salida, observa que debes agregar a self los \n",
        "      objetos nn.Linear que desees usar, en este caso sólo es uno.\n",
        "      '''\n",
        "      super(Perceptron, self).__init__()\n",
        "      self.fc1 = nn.Linear(in_features = input_size, out_features = output_size, bias=bias)\n",
        "\n",
        "    def forward(self, inputX):\n",
        "      '''Debes sobreescibir el método forward, el cual recibe únicamente una entrada:\n",
        "      los valores sobre los que se evaluará la red.\n",
        "      Para hacer pasar la entrada a través de una capa, sólo debes llamarla con la entrada\n",
        "      como parámetro.\n",
        "      Finalmente puedes notar que se puede aplicar una función de activación a\n",
        "      todos los valores resultantes.\n",
        "      Este método debe resultar en, al menos una salida.\n",
        "      '''\n",
        "      out = torch.sigmoid(self.fc1(inputX))\n",
        "      return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLtNcAWIwFfJ"
      },
      "source": [
        "Con la clase implementada ahora sólo es necesario definir la manera en la que se realiza el entrenamiento.\n",
        "\n",
        "Puedes implementar un método train que reciba los parámetros necesarios, entre los cuales se encuentra:\n",
        "\n",
        "*   El modelo que se desea entrenar (debe heredar de nn.Module).\n",
        "*   El número de epocas de entrenamiento.\n",
        "*   Los datos de entrenamiento (entradas) y sus etiquetas.\n",
        "*   El criterio de oprimización, es decir, la función de costo, las cuales puedes consultar en [Pytorch loss functions.](https://pytorch.org/docs/stable/nn.html#loss-functions)\n",
        "*   El método de optimizacion, por ejemplo SGD, distintos algoritmos se encuentran implementados en el modulo [torch.optim](https://pytorch.org/docs/stable/optim.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LYaDo0v0wEr2"
      },
      "outputs": [],
      "source": [
        "def train(net, epochs, data, labels, criterion, optimizer, cuda=False):\n",
        "    '''Entrena la red net, por un numero de epocas \"epochs\",\n",
        "    usando como función de pérdida la definida en \"criterion\" y el\n",
        "    optimizador pasado como parámetro.'''\n",
        "    avg_loss = torch.Tensor()\n",
        "    tqdm_epochs = tnrange(epochs)\n",
        "    for epoch in tqdm_epochs:\n",
        "        for d,label in zip(data,labels):\n",
        "            if(cuda and torch.cuda.is_available()): #Si nuestra PC cuenta con GPU,realizamos cálculos en ella\n",
        "                d = d.cuda()\n",
        "                label = label.cuda()\n",
        "    \n",
        "            optimizer.zero_grad() #limpiamos los gradientes actuales\n",
        "            output = net(d)       #llamar a nuestro objeto red con parámetros es introducirlos a la red\n",
        "            #print(\"o\", output)\n",
        "            #print(\"l\", label)\n",
        "            loss = criterion(output, label) #calculamos el error de nuestro modelo   \n",
        "            loss.backward()       #calculamos el gradiente del error y se almacena dentro del modelo\n",
        "            optimizer.step()      #usando los datos almacenados y el método seleccionado actualizamos los parámetros de la red\n",
        "\n",
        "            avg_loss = torch.cat([avg_loss, torch.Tensor([loss])],0)\n",
        "\n",
        "    tqdm_epochs.set_description(\"Loss %.6f\"%(avg_loss.sum()/avg_loss.numel()).item())  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xelgxu5V2rE4"
      },
      "source": [
        "Para nuestro ejemplo podemos tratar el problema de aprender la compuerta AND como un problema de regresión, para lo cual puedes usar la función de error min squared error (MSE), que se encuentra implementada como MSELoss().\n",
        "\n",
        "Además podemos utilizar descenso por el gradiente, que puedes encontrar en torch.optim.SGD. A este objeto necesitas\n",
        "pasarle como argumento los parámetros (pesos) del modelo a optimizar, lo cual puedes lograr con el método parameters() de los objetos que heredan de nn.Module; en el caso del descenso por el gradiente tambien debes especificar la taza de aprendizaje.\n",
        "\n",
        "Puedes ejecutar el entrenamiento sobre el modelo de 2 entradas y una única salida pasándolo como argumento al método train."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WFkhWi2Lwp-A"
      },
      "outputs": [],
      "source": [
        "# Entradas\n",
        "X_AND = torch.Tensor([[0,0],[0,1],[1,0],[1,1]])\n",
        "# Etiquetas\n",
        "Y_AND = torch.Tensor([0,0,0,1])\n",
        "Perceptron_AND = Perceptron(2,1)\n",
        "criterio  = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(Perceptron_AND.parameters(), lr=0.1)\n",
        "train(Perceptron_AND, 10000, X_AND, Y_AND, criterio, optimizer)\n",
        "#train(Perceptron_AND, 2, X_AND, Y_AND, criterio, optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJBFGm6A3-Aq"
      },
      "source": [
        "Ahora para realizar predicciones con el modelo obtenido unicamente debes pasar los datos como parámetro a la red, la cual ejecutará de manera automática el método forward.\n",
        "Dado que el problema fue modelado como regresión observarás que los primeros 3 valores son muy cercanos a 0 y el último tiene un valor muy cercano a 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gT1IaZZ84HUw"
      },
      "outputs": [],
      "source": [
        "resultados = Perceptron_AND(X_AND)\n",
        "print(resultados)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Yv_M-M6-Oez"
      },
      "source": [
        "## Dos perceptrones en una capa\n",
        "\n",
        "Si quisiéramos resolverlo como un problema de clasificación debemos realizar dos cambios:\n",
        "\n",
        "*   Cambiar el formato de las etiquetas para que se encuentren en [one-hot](https://en.wikipedia.org/wiki/One-hot).  El primer perceptrón representará al 0 y el segundo, al 1.\n",
        "*   Usar una función de costo que evalúe el error de clasificación, como la entropía cruzada binaria, implementada en PyTorch como BCELoss()."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aX4fCyJr5OSl"
      },
      "outputs": [],
      "source": [
        "Y_one_hot = torch.Tensor([[1,0],[1,0],[1,0],[0,1]])\n",
        "Perceptron_AND_C = Perceptron(2,2)\n",
        "criterio_clasificacion =  nn.BCELoss()\n",
        "optimizer = torch.optim.SGD(Perceptron_AND_C.parameters(), lr=0.1)\n",
        "train(Perceptron_AND_C, 50000, X_AND, Y_one_hot, criterio_clasificacion, optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRm-caJH_MDP"
      },
      "source": [
        "Nota que el resultado será alimentado a la funcion Softmax para que puedas interpretar los resultados como probabilidades.  La salida con el valor más alto es la clase ganadora."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uVLuPXEg68OB"
      },
      "outputs": [],
      "source": [
        "resultados_clasificacion = Perceptron_AND_C(X_AND)\n",
        "print(nn.functional.softmax(resultados_clasificacion,dim=0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQmEcw0YpG9h"
      },
      "source": [
        "## Perceptrón multicapa\n",
        "\n",
        "### Ejercicio 1: XOR\n",
        "\n",
        "Haciendo uso de un perceptrón multicapa (MLP, por sus siglas en inglés) crea un modelo capaz de simular la compuerta logica XOR considerandolo un problema de regresión.\n",
        "\n",
        "Para esta tarea debes usar tres capas:\n",
        "* La capa de entrada (los valores de entrada a la compuerta)\n",
        "* Una capa oculta con 3 unidades\n",
        "* Una capa de salida con una unica neurona \n",
        "\n",
        "Deberas especificar una **taza de aprendizaje** adecuada así como el número de iteraciones necesario para lograr el aprendizaje."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RRC5XVB6UyJu"
      },
      "outputs": [],
      "source": [
        "X_XOR = torch.Tensor([[0,0],\n",
        "                      [0,1],\n",
        "                      [1,0],\n",
        "                      [1,1]])\n",
        "Y_XOR = torch.Tensor([0,1,1,0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ulVM8HcoVip5"
      },
      "outputs": [],
      "source": [
        "class XOR(nn.Module):\n",
        "    def __init__(self, input_layer_size, hidden_layer_size, output_layer_size, bias=True):\n",
        "        super(XOR,self).__init__()\n",
        "        self.hidden = nn.Linear(input_layer_size, hidden_layer_size, bias = bias)\n",
        "        self.out = nn.Linear(hidden_layer_size, output_layer_size, bias = bias)\n",
        "        self.act_func = nn.Tanh()\n",
        "\n",
        "    def forward(self, x):\n",
        "      x = self.hidden(x)\n",
        "      x = self.act_func(x)\n",
        "      x = self.out(x)\n",
        "      x = self.act_func(x)\n",
        "      return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uyDBhj6NwyIb"
      },
      "outputs": [],
      "source": [
        "XORNet = XOR(2,3,1)\n",
        "optimizer  = torch.optim.Adam(XORNet.parameters(), lr=0.1)\n",
        "criterio = nn.MultiLabelSoftMarginLoss()\n",
        "train(XORNet, 1000, X_XOR, Y_XOR,criterio, optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kIM-zX_5ycaG"
      },
      "outputs": [],
      "source": [
        "predicciones= XORNet(X_XOR)\n",
        "print(predicciones)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59mYQEaUyPLo"
      },
      "source": [
        "### Ejercicio 2:\n",
        "\n",
        "Encuentra un modelo para clasificar los datos contenidos en Xs y Ys, para lo cual debes encontrar el numero de capas y neuronas adecuado, así como una taza de aprendizaje.\n",
        "\n",
        "HINT: Puedes utilizar [Playground.tensorflow](https://playground.tensorflow.org) para buscar gráficamente los modelos y al final implementarlos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kGBvQYnn0KoZ"
      },
      "outputs": [],
      "source": [
        "def to_learn1(x,y):\n",
        "    return 1 if x * y >= 0 else -1;\n",
        "\n",
        "def asignar_color(clase):\n",
        "    if(clase==1):\n",
        "        return 'r'\n",
        "    elif(clase==-1):\n",
        "        return 'b'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILQGbMahyRGS"
      },
      "source": [
        "# Observaciones\n",
        "En este ejercicio se modificaron los conjuntos de datos por conveniencia, añadiendoles más elementos. Es posible que en el primer entrenamiento la exactitud de las predicciones disminuya hasta a un 80%. Por tanto, en caso de resultar así, en el primer entrenamiento, se recomienda volver a generar el conjunto de datos, el modelo y volverlo a entrenar. La tasa máxima de exactitud fue del 100%.\n",
        "\n",
        "Por otro lado, dado el error **RuntimeError: expected scalar type Long but found float**, no se pudo utilizar la función de perdida **nn.BCELoss()**, se trato de resolver el problema con investigación y consultando a los ayudantes y profesora, pero no fue posible corregirlo. Esto mismo aplica para el ejercicio tres. De ahí la utilización de las funciones: **nn.MultiLabelSoftMarginLoss()** y **nn.SoftMarginLoss()**. Se considera que esta es la razón de la falta de ajuste en el entrenamiento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DSpedQNjwEtj"
      },
      "outputs": [],
      "source": [
        "# Características de entrada y salida\n",
        "Xs = [random.uniform(-5,5) for _ in range(0,200)] # Puede ajustar para más elementos, no menos\n",
        "Ys = [random.uniform(-5,5) for _ in range(0,200)]\n",
        "Zs = [to_learn1(punto[0],punto[1]) for punto in zip(Xs,Ys)]\n",
        "Colores = [asignar_color(p) for p in Zs]\n",
        "XYs = [[a, b] for a, b in zip(Xs, Ys)]\n",
        "X_features1 = torch.Tensor(XYs)\n",
        "Y_features1 = torch.tensor(Zs)\n",
        "plt.scatter(Xs,Ys,c=Colores)\n",
        "plt.plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kAoWZCypCo68"
      },
      "outputs": [],
      "source": [
        "class Exercise2(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Exercise2,self).__init__()\n",
        "    self.hidden_layer1 = nn.Linear(2, 8)\n",
        "    self.hidden_layer2 = nn.Linear(8, 4)\n",
        "    self.output_layer = nn.Linear(4, 1)\n",
        "    self.act_func = nn.Tanh()\n",
        "  \n",
        "  def forward(self, x):\n",
        "     x = self.hidden_layer1(x)\n",
        "     x = self.act_func(x)\n",
        "     x = self.hidden_layer2(x)\n",
        "     x = self.act_func(x)\n",
        "     x = self.output_layer(x)\n",
        "     x = self.act_func(x)\n",
        "     return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LpVPprWkfCjc"
      },
      "outputs": [],
      "source": [
        "# Función que calcula de exactitud de un modelo\n",
        "def accuracy_fn(y_true, y_pred):\n",
        "  counter = 0\n",
        "  for yt,yp in zip(y_true, y_pred):\n",
        "    if yt==yp[0]:\n",
        "      counter = counter+1\n",
        "  return (counter/len(y_true))*100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i1XyzoAQQQN9"
      },
      "outputs": [],
      "source": [
        "# Creamos al modelo, definimos la función de perdida y el optimizador, así como la tasa de aprendizaje\n",
        "model1 = Exercise2()\n",
        "criterio1 = nn.SoftMarginLoss()\n",
        "optimizer1 = torch.optim.Adam(model1.parameters(), lr=0.01)\n",
        "plot_decision_boundary(model1,X_features1, Y_features1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4PludRRMUA-L"
      },
      "outputs": [],
      "source": [
        "# Entrenamos al modelo\n",
        "train(model1, 1000, X_features1, Y_features1, criterio1, optimizer1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qh4TkKYmqh8c"
      },
      "outputs": [],
      "source": [
        "# Probamos el modelo\n",
        "with torch.inference_mode():\n",
        "  predicciones1= model1(X_features1)\n",
        "plot_decision_boundary(model1, X_features1, Y_features1)\n",
        "print(\"Exactitud: \", accuracy_fn(Y_features1, predicciones1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2zA-PrEz-rL"
      },
      "source": [
        "### Ejercicio 3:\n",
        "\n",
        "Encuentra un modelo para clasificar los datos contenidos en X2s y Y2s, para lo cual debes encontrar el numero de capas y neuronas adecuado, así como una taza de aprendizaje.\n",
        "\n",
        "# Observaciones\n",
        "Este modelo pretende ajustarse al modelo que clasifica el conjunto de datos del playground. Como se menciono antes, es probable que el uso de las funciones de perdida antes mencionadas, en lugar de alguna otras más eficientes, para este tipo de conjunto de datos pueda indicar la disminución del 'aprendizaje'.\n",
        "\n",
        "La tasa de exactitud máxima obtenida en las pruebas fue del 64%, y la mínima del 50% , lo cual se le atribuye a lo antes mencionado. En caso de requerir más información, puede consultar el archivo **readme.md**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fdNdIRCN0AEo"
      },
      "outputs": [],
      "source": [
        "def to_learn2(deltaT,label):\n",
        "    n=100\n",
        "    Xs = []\n",
        "    Ys = []\n",
        "    Cs = [label]*n\n",
        "    for i in range(0,n):\n",
        "        r = i / n * 20;\n",
        "        t = 1.3 * i / n * 2 * math.pi + deltaT;\n",
        "        x = r * math.sin(t) + random.uniform(-1, 1);\n",
        "        y = r * math.cos(t) + random.uniform(-1, 1);\n",
        "        Xs.append(x)\n",
        "        Ys.append(y)\n",
        "    return Xs,Ys,Cs\n",
        "\n",
        "def create_data():\n",
        "    x1,y1,cs1 = to_learn2(0,-1)\n",
        "    x2,y2,cs2 = to_learn2(math.pi,1)\n",
        "    x1.extend(x2)\n",
        "    y1.extend(y2)\n",
        "    cs1.extend(cs2)\n",
        "    colores=[asignar_color(p) for p in cs1]\n",
        "    return x1,y1,cs1,colores\n",
        "\n",
        "X1, X2,clases,colores = create_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bcMuzISMuF_2"
      },
      "outputs": [],
      "source": [
        "# Generamos los tensores para las características de entrada y salida\n",
        "Y_features2 = torch.Tensor(clases)\n",
        "X_features2 = torch.Tensor([[a, b] for a, b in zip(X1, X2)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l0Ch5Yp9Cj1G"
      },
      "outputs": [],
      "source": [
        "class Exercise3(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Exercise3,self).__init__()\n",
        "    self.hidden_layer1 = nn.Linear(7, 16)\n",
        "    self.hidden_layer2 = nn.Linear(16, 8)\n",
        "    self.output_layer = nn.Linear(8, 1)\n",
        "    self.act_func = nn.Tanh()\n",
        "  \n",
        "  def forward(self, x):\n",
        "    #print(x)\n",
        "    if isMatrix(x.tolist()) :\n",
        "     x = addFeaturesList(x)\n",
        "    else:\n",
        "     x = addFeaturesTuple(x)\n",
        "    x = self.hidden_layer1(x)\n",
        "    x = self.act_func(x)\n",
        "    x = self.hidden_layer2(x)\n",
        "    x = self.act_func(x)\n",
        "    x = self.output_layer(x)\n",
        "    x = self.act_func(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1k0BVBxOzzH0"
      },
      "outputs": [],
      "source": [
        "# Creamos al modelo, definimos la función de perdida y el optimizador, así como la tasa de aprendizaje\n",
        "model2 = Exercise3()\n",
        "criterio2 = nn.MultiLabelSoftMarginLoss()\n",
        "optimizer2 = torch.optim.Adam(model2.parameters(), lr=0.01)\n",
        "#predicciones2 = model2(X_features2)\n",
        "#print(\"exactitud: \", accuracy_fn(Y_features2, predicciones2))\n",
        "plot_decision_boundary(model2,X_features2, Y_features2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "efL-HaynCaNm"
      },
      "outputs": [],
      "source": [
        "# Entrenamos el modelo\n",
        "train(model2, 2000, X_features2, Y_features2, criterio2, optimizer2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cvYJoe0QCwEY"
      },
      "outputs": [],
      "source": [
        "# Make predictions on the test data\n",
        "with torch.inference_mode():\n",
        "  predicciones2= model2(X_features2)\n",
        "#print(predicciones2)\n",
        "#print(Y_features2)\n",
        "plot_decision_boundary(model1, X_features2, Y_features2)\n",
        "print(\"exactitud: \", accuracy_fn(Y_features2, predicciones2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NlBCeK7qAd6y"
      },
      "outputs": [],
      "source": [
        "# Funcion que calcula las caracteríticas para una lista de listas de dos elementos\n",
        "# X son las caracteristicas de entrada\n",
        "def addFeaturesList(X):\n",
        "  X_list = X.tolist()\n",
        "  #print(\"Caract: \",X_list)\n",
        "  X1 = [x[0] for x in X_list]\n",
        "  X2 = [x[1] for x in X_list]\n",
        "  X1_2 = list(map(lambda x: x ** 2, X1))\n",
        "  X2_2 = list(map(lambda x: x ** 2, X2))\n",
        "  X1X2 = list(map(lambda x, y: x * y, X1, X2))\n",
        "  X1_sen = list(map(lambda x: math.sin(x), X1))\n",
        "  X2_sen = list(map(lambda x: math.sin(x), X2))\n",
        "  return torch.Tensor([[a, b, c, d, f, g, h] for a, b, c, d, f, g, h in zip(X1, X2, X1_2, X2_2, X1X2, X1_sen, X2_sen)])\n",
        "\n",
        "# Funcion que calcula las caracteríticas para una lista de dos elementos\n",
        "def addFeaturesTuple(X):\n",
        "  X_list = X.tolist()\n",
        "  X1 = X_list[0]\n",
        "  X2 = X_list[1]\n",
        "  X1_2 = X1 ** 2\n",
        "  X2_2 = X2 ** 2\n",
        "  X1X2 = X1*X2\n",
        "  X1_sen = math.sin(X1)\n",
        "  X2_sen = math.sin(X2)\n",
        "  return torch.Tensor([X1, X2, X1_2, X2_2, X1X2, X1_sen, X2_sen])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "s8waV7ozUEzJ"
      },
      "outputs": [],
      "source": [
        "# Función que determina si una entrada es una matriz\n",
        "def isMatrix(X):\n",
        "  if type(X) == list and all(isinstance(i, list) for i in X):\n",
        "    return True\n",
        "  else:\n",
        "    return False"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
